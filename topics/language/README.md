# Language Models
- [JMLR 2023] **PaLM: Scaling Language Modeling with Pathways** [[paper]](https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf)

### Mixture of Experts (MoE)
- [ArXiv 2024] **Mixtral of Experts** [[paper]](https://arxiv.org/pdf/2401.04088.pdf)
- [JMLR 2022] **Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity** [[paper]](https://arxiv.org/pdf/2101.03961.pdf)
- [ICLR 2021] **GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding** [[paper]](https://arxiv.org/pdf/2006.16668.pdf)

### NN Architectures
- [ArXiv 2023] **Mamba: Linear-Time Sequence Modeling with Selective State Spaces** [[paper]](https://arxiv.org/pdf/2312.00752.pdf) [[code]](https://github.com/state-spaces/mamba)
- [NeurIPS 2020] **ReZero is All You Need: Fast Convergence at Large Depth** [[paper]](https://arxiv.org/pdf/2003.04887.pdf)

### Engineering
- [ICLR 2024] **FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning** [[paper]](https://arxiv.org/pdf/2307.08691.pdf)
- [ACM Computing Surveys 2022] **Efficient Transformers: A Survey** [[paper]](https://arxiv.org/pdf/2009.06732.pdf)
- [ArXiv 2022] **Self-attention Does Not Need O(n2) Memory** [[paper]](https://arxiv.org/pdf/2112.05682.pdf)
- [NeurIPS 2022] **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness** [[paper]](https://arxiv.org/pdf/2205.14135.pdf) [[code]](https://github.com/Dao-AILab/flash-attention)
- [ArXiv 2019] **A Study of BFLOAT16 for Deep Learning Training** [[paper]](https://arxiv.org/pdf/1905.12322.pdf)
- [ArXiv 2018] **Online Normalizer Calculation for Softmax** [[paper]](https://arxiv.org/pdf/1805.02867.pdf)