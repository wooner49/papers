# Language Models
- [JMLR 2023] **PaLM: Scaling Language Modeling with Pathways** [[paper]](https://www.jmlr.org/papers/volume24/22-1144/22-1144.pdf)
- [ICLR 2022] **Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation** [[paper]](https://arxiv.org/pdf/2108.12409.pdf) - ALiBi

### Mixture of Experts (MoE)
- [ArXiv 2024] **Mixtral of Experts** [[paper]](https://arxiv.org/pdf/2401.04088.pdf)
- [JMLR 2022] **Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity** [[paper]](https://arxiv.org/pdf/2101.03961.pdf)
- [ICLR 2021] **GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding** [[paper]](https://arxiv.org/pdf/2006.16668.pdf)

### State Space Models (SSMs)
- [ArXiv 2023] **Mamba: Linear-Time Sequence Modeling with Selective State Spaces** [[paper]](https://arxiv.org/pdf/2312.00752.pdf) [[ICLR-version]](https://openreview.net/pdf?id=AL1fq05o7H) [[code]](https://github.com/state-spaces/mamba)
- [ICLR 2022] **Efficiently Modeling Long Sequences with Structured State Spaces** [[paper]](https://arxiv.org/pdf/2111.00396.pdf) - S4
- [NeurIPS 2021] **Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers** [[paper]](https://arxiv.org/pdf/2110.13985.pdf) - LSSL
- [NeurIPS 2020] **HiPPO: Recurrent Memory with Optimal Polynomial Projections** [[paper]](https://arxiv.org/pdf/2008.07669.pdf) - HiPPO
- [NeurIPS 2019] **Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks** [[paper]](https://papers.nips.cc/paper_files/paper/2019/file/952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf) [[code]](https://github.com/nengo/keras-lmu)


### Engineering
- [ICLR 2024] **FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning** [[paper]](https://arxiv.org/pdf/2307.08691.pdf)
- [ACM Computing Surveys 2022] **Efficient Transformers: A Survey** [[paper]](https://arxiv.org/pdf/2009.06732.pdf)
- [ArXiv 2022] **Self-attention Does Not Need O(n2) Memory** [[paper]](https://arxiv.org/pdf/2112.05682.pdf)
- [NeurIPS 2022] **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness** [[paper]](https://arxiv.org/pdf/2205.14135.pdf) [[code]](https://github.com/Dao-AILab/flash-attention)
- [ArXiv 2019] **A Study of BFLOAT16 for Deep Learning Training** [[paper]](https://arxiv.org/pdf/1905.12322.pdf)
- [ArXiv 2018] **Online Normalizer Calculation for Softmax** [[paper]](https://arxiv.org/pdf/1805.02867.pdf)






## asdf
- [NeurIPS 2020] **ReZero is All You Need: Fast Convergence at Large Depth** [[paper]](https://arxiv.org/pdf/2003.04887.pdf)